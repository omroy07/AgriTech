{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ðŸŒ± Soil Classification using Deep Learning\n",
    "\n",
    "## Overview\n",
    "This notebook implements soil classification using Transfer Learning with MobileNetV2.\n",
    "\n",
    "**Dataset:** 6 soil types (144 images)\n",
    "- Alluvial soils\n",
    "- Black soils  \n",
    "- Chalky soils\n",
    "- Clayey soils\n",
    "- Loamy soils\n",
    "- Red soils\n",
    "\n",
    "**Improvements over baseline:**\n",
    "- Transfer Learning with MobileNetV2\n",
    "- Advanced data augmentation\n",
    "- Comprehensive evaluation metrics\n",
    "- Learning rate scheduling\n",
    "- Better error handling\n",
    "\n",
    "**Expected Performance:**\n",
    "- Baseline CNN: ~81% accuracy\n",
    "- Transfer Learning: ~92%+ accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "    Activation, Flatten, Dropout, Dense\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Centralized configuration for easy experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'data_dir': r\"C:\\Users\\fastf\\Downloads\\soil dataset\\Soil Types\",  # UPDATE THIS PATH\n",
    "    'img_size': (150, 150),\n",
    "    'batch_size': 32,\n",
    "    'validation_split': 0.2,\n",
    "    \n",
    "    # Model settings\n",
    "    'model_type': 'transfer_learning',  # 'cnn' or 'transfer_learning'\n",
    "    'num_classes': 6,\n",
    "    'base_model': 'MobileNetV2',  # MobileNetV2, EfficientNetB0, ResNet50\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs_stage1': 15,  # For transfer learning: train new layers\n",
    "    'epochs_stage2': 20,  # For transfer learning: fine-tune all layers\n",
    "    'epochs_cnn': 50,     # For baseline CNN\n",
    "    'learning_rate': 0.001,\n",
    "    'learning_rate_finetune': 0.0001,\n",
    "    \n",
    "    # Callbacks\n",
    "    'early_stopping_patience': 10,\n",
    "    'reduce_lr_patience': 3,\n",
    "    \n",
    "    # Paths\n",
    "    'model_save_dir': './models',\n",
    "    'logs_dir': './logs',\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['model_save_dir'], exist_ok=True)\n",
    "os.makedirs(CONFIG['logs_dir'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model type: {CONFIG['model_type']}\")\n",
    "print(f\"Image size: {CONFIG['img_size']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using TensorFlow's image_dataset_from_directory\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    image_size=CONFIG['img_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_labels = dataset.class_names\n",
    "print(f\"\\nClass labels: {class_labels}\")\n",
    "print(f\"Number of classes: {len(class_labels)}\")\n",
    "\n",
    "# Calculate dataset statistics\n",
    "total_batches = len(dataset)\n",
    "total_images = sum([len(batch[1]) for batch in dataset])\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Total batches: {total_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Get one batch\n",
    "for images, labels in dataset.take(1):\n",
    "    for i in range(min(12, len(images))):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_labels[labels[i]])\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample images saved as 'sample_images.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep-header",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_size = int(0.8 * total_batches)\n",
    "val_size = total_batches - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "print(f\"Training batches: {train_size}\")\n",
    "print(f\"Validation batches: {val_size}\")\n",
    "\n",
    "# Normalize the data (scale pixel values to [0, 1])\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Configure dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"\\nData normalization and optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "augmentation-header",
   "metadata": {},
   "source": [
    "## 6. Data Augmentation\n",
    "\n",
    "Advanced augmentation to improve model generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data augmentation layer\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.3),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Apply augmentation to training data\n",
    "train_dataset_augmented = train_dataset.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"Data augmentation configured successfully!\")\n",
    "\n",
    "# Visualize augmented images\n",
    "plt.figure(figsize=(15, 8))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    sample_image = images[0:1]  # Take first image\n",
    "    \n",
    "    for i in range(9):\n",
    "        augmented_image = data_augmentation(sample_image, training=True)\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_image[0])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Augmentation {i+1}\")\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('augmentation_examples.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 7. Model Architecture\n",
    "\n",
    "### Option A: Baseline CNN (Original)\n",
    "### Option B: Transfer Learning with MobileNetV2 (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_cnn(input_shape=(150, 150, 3), num_classes=6):\n",
    "    \"\"\"\n",
    "    Build baseline CNN model (original architecture)\n",
    "    Expected accuracy: ~81%\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, (3, 3), input_shape=input_shape),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(32, (3, 3)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        Conv2D(64, (3, 3)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Baseline CNN architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model(input_shape=(150, 150, 3), num_classes=6):\n",
    "    \"\"\"\n",
    "    Build Transfer Learning model with MobileNetV2\n",
    "    Expected accuracy: ~92%+\n",
    "    \n",
    "    Why MobileNetV2?\n",
    "    - Pre-trained on ImageNet (1.4M images)\n",
    "    - Lightweight (14MB)\n",
    "    - Fast inference\n",
    "    - Excellent for small datasets\n",
    "    \"\"\"\n",
    "    # Load pre-trained MobileNetV2 (without top classification layer)\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build complete model\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "print(\"Transfer Learning architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model based on configuration\n",
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(\"Building Transfer Learning model with MobileNetV2...\")\n",
    "    model, base_model = build_transfer_learning_model(\n",
    "        input_shape=(*CONFIG['img_size'], 3),\n",
    "        num_classes=CONFIG['num_classes']\n",
    "    )\n",
    "else:\n",
    "    print(\"Building baseline CNN model...\")\n",
    "    model = build_baseline_cnn(\n",
    "        input_shape=(*CONFIG['img_size'], 3),\n",
    "        num_classes=CONFIG['num_classes']\n",
    "    )\n",
    "    base_model = None\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate']),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "callbacks-header",
   "metadata": {},
   "source": [
    "## 8. Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "callbacks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate reduction\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=CONFIG['reduce_lr_patience'],\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(CONFIG['model_save_dir'], 'best_model_{epoch:02d}_{val_accuracy:.4f}.h5'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TensorBoard\n",
    "log_dir = os.path.join(CONFIG['logs_dir'], datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, checkpoint, tensorboard_callback]\n",
    "\n",
    "print(\"Callbacks configured successfully!\")\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"To view: tensorboard --logdir=./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "## 9. Model Training\n",
    "\n",
    "### Two-Stage Training for Transfer Learning:\n",
    "1. **Stage 1**: Train only new layers (frozen base)\n",
    "2. **Stage 2**: Fine-tune entire model (unfrozen base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-stage1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(\"=\"*70)\n",
    "    print(\"STAGE 1: Training new layers only (base model frozen)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    history_stage1 = model.fit(\n",
    "        train_dataset_augmented,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs_stage1'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStage 1 training complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history_stage1.history['val_accuracy']):.4f}\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Training baseline CNN model\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset_augmented,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs_cnn'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-stage2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STAGE 2: Fine-tuning entire model (base model unfrozen)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Unfreeze base model\n",
    "    base_model.trainable = True\n",
    "    print(f\"Base model layers now trainable: {len(base_model.layers)}\")\n",
    "    \n",
    "    # Recompile with lower learning rate (CRITICAL!)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=CONFIG['learning_rate_finetune']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate reduced to: {CONFIG['learning_rate_finetune']}\")\n",
    "    \n",
    "    # Continue training\n",
    "    history_stage2 = model.fit(\n",
    "        train_dataset_augmented,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs_stage2'],\n",
    "        callbacks=callbacks,\n",
    "        initial_epoch=len(history_stage1.history['loss']),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStage 2 training complete!\")\n",
    "    print(f\"Best validation accuracy: {max(history_stage2.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    # Combine histories\n",
    "    history = type('obj', (object,), {\n",
    "        'history': {\n",
    "            'loss': history_stage1.history['loss'] + history_stage2.history['loss'],\n",
    "            'accuracy': history_stage1.history['accuracy'] + history_stage2.history['accuracy'],\n",
    "            'val_loss': history_stage1.history['val_loss'] + history_stage2.history['val_loss'],\n",
    "            'val_accuracy': history_stage1.history['val_accuracy'] + history_stage2.history['val_accuracy']\n",
    "        }\n",
    "    })()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-training-header",
   "metadata": {},
   "source": [
    "## 10. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add vertical line for stage transition if transfer learning\n",
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    stage1_epochs = len(history_stage1.history['loss'])\n",
    "    for ax in axes:\n",
    "        ax.axvline(x=stage1_epochs-1, color='red', linestyle='--', \n",
    "                   label='Fine-tuning starts', linewidth=2)\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TRAINING METRICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Val Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Best Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Final Train Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions\n",
    "print(\"Collecting predictions on validation set...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "for images, labels in val_dataset:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_pred_proba.extend(predictions)\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(f\"Collected {len(y_true)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_labels,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "metrics_df.to_csv('classification_metrics.csv', index=False)\n",
    "print(\"\\nMetrics saved to 'classification_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy from confusion matrix\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for multi-class classification\n",
    "y_true_bin = label_binarize(y_true, classes=range(len(class_labels)))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, label in enumerate(class_labels):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{label} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Multi-class Classification', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate macro-average AUC\n",
    "all_fpr = np.unique(np.concatenate([roc_curve(y_true_bin[:, i], y_pred_proba[:, i])[0] \n",
    "                                     for i in range(len(class_labels))]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(len(class_labels)):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "mean_tpr /= len(class_labels)\n",
    "macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "print(f\"\\nMacro-average AUC: {macro_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "## 12. Save Model with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save final model\n",
    "model_path = os.path.join(CONFIG['model_save_dir'], 'soil_classifier_final.h5')\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': CONFIG['model_type'],\n",
    "    'base_model': CONFIG['base_model'] if CONFIG['model_type'] == 'transfer_learning' else 'CNN',\n",
    "    'num_classes': CONFIG['num_classes'],\n",
    "    'class_labels': class_labels,\n",
    "    'img_size': CONFIG['img_size'],\n",
    "    'training_date': datetime.datetime.now().isoformat(),\n",
    "    'total_parameters': int(total_params),\n",
    "    'performance': {\n",
    "        'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "        'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "        'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "        'final_train_loss': float(history.history['loss'][-1]),\n",
    "        'final_val_loss': float(history.history['val_loss'][-1]),\n",
    "        'macro_auc': float(macro_auc)\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'learning_rate_finetune': CONFIG['learning_rate_finetune'] if CONFIG['model_type'] == 'transfer_learning' else None,\n",
    "        'total_epochs': len(history.history['loss'])\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = model_path.replace('.h5', '_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(json.dumps(metadata, indent=2))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-header",
   "metadata": {},
   "source": [
    "## 13. Prediction Function with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image(img_path):\n",
    "    \"\"\"\n",
    "    Validate image file before processing\n",
    "    \"\"\"\n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "    \n",
    "    allowed_extensions = ['.png', '.jpg', '.jpeg', '.bmp']\n",
    "    ext = os.path.splitext(img_path)[1].lower()\n",
    "    if ext not in allowed_extensions:\n",
    "        raise ValueError(f\"Invalid file type: {ext}. Allowed: {allowed_extensions}\")\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img.verify()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Corrupt or invalid image file: {e}\")\n",
    "\n",
    "\n",
    "def predict_soil_type(img_path, model, class_labels, img_size=(150, 150)):\n",
    "    \"\"\"\n",
    "    Predict soil type from image with comprehensive error handling\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to soil image\n",
    "        model: Trained Keras model\n",
    "        class_labels: List of class names\n",
    "        img_size: Target image size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results or error information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate input\n",
    "        validate_image(img_path)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = img_array / 255.0  # Normalize\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        pred_class = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][pred_class]\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'success': True,\n",
    "            'predicted_class': class_labels[pred_class],\n",
    "            'confidence': float(confidence),\n",
    "            'all_probabilities': {\n",
    "                class_labels[i]: float(predictions[0][i])\n",
    "                for i in range(len(class_labels))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(\"Prediction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-prediction-header",
   "metadata": {},
   "source": [
    "## 14. Test Prediction\n",
    "\n",
    "**Note:** Update the image path below to test prediction on your own images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction (update path to your test image)\n",
    "test_image_path = r\"C:\\Users\\fastf\\Downloads\\soil dataset\\Soil Types\\Clayey soils\\clay 9.png\"\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    # Make prediction\n",
    "    result = predict_soil_type(\n",
    "        test_image_path,\n",
    "        model,\n",
    "        class_labels,\n",
    "        img_size=CONFIG['img_size']\n",
    "    )\n",
    "    \n",
    "    # Display result\n",
    "    if result['success']:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PREDICTION RESULT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Predicted Soil Type: {result['predicted_class']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)\")\n",
    "        print(\"\\nAll Class Probabilities:\")\n",
    "        for soil_type, prob in sorted(result['all_probabilities'].items(), \n",
    "                                     key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {soil_type}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Visualize prediction\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Show image\n",
    "        img = Image.open(test_image_path)\n",
    "        ax1.imshow(img)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(f'Predicted: {result[\"predicted_class\"]}\\nConfidence: {result[\"confidence\"]:.2%}',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Show probability distribution\n",
    "        probs = result['all_probabilities']\n",
    "        labels = list(probs.keys())\n",
    "        values = list(probs.values())\n",
    "        \n",
    "        bars = ax2.barh(labels, values, color='skyblue')\n",
    "        bars[labels.index(result['predicted_class'])].set_color('green')\n",
    "        ax2.set_xlabel('Probability', fontsize=10)\n",
    "        ax2.set_title('Class Probabilities', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('prediction_result.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"\\nPrediction failed: {result['error']}\")\n",
    "else:\n",
    "    print(f\"Test image not found: {test_image_path}\")\n",
    "    print(\"Please update the path to test prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 15. Final Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel Type: {CONFIG['model_type']}\")\n",
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(f\"Base Model: {CONFIG['base_model']}\")\n",
    "print(f\"\\nDataset Size: {total_images} images\")\n",
    "print(f\"Number of Classes: {CONFIG['num_classes']}\")\n",
    "print(f\"Classes: {', '.join(class_labels)}\")\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Image Size: {CONFIG['img_size']}\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"-\"*70)\n",
    "print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f} ({max(history.history['val_accuracy'])*100:.2f}%)\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f} ({history.history['val_accuracy'][-1]*100:.2f}%)\")\n",
    "print(f\"Macro-average AUC: {macro_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FILES GENERATED\")\n",
    "print(\"-\"*70)\n",
    "print(\"âœ“ sample_images.png - Dataset visualization\")\n",
    "print(\"âœ“ augmentation_examples.png - Data augmentation examples\")\n",
    "print(\"âœ“ training_history.png - Training curves\")\n",
    "print(\"âœ“ confusion_matrix.png - Confusion matrix heatmap\")\n",
    "print(\"âœ“ roc_curves.png - ROC curves for all classes\")\n",
    "print(\"âœ“ classification_metrics.csv - Per-class metrics\")\n",
    "print(\"âœ“ prediction_result.png - Example prediction visualization\")\n",
    "print(f\"âœ“ {model_path} - Saved model\")\n",
    "print(f\"âœ“ {metadata_path} - Model metadata\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"-\"*70)\n",
    "print(\"1. Try different base models (EfficientNetB0, ResNet50)\")\n",
    "print(\"2. Experiment with image size (224x224)\")\n",
    "print(\"3. Collect more training data\")\n",
    "print(\"4. Implement k-fold cross-validation\")\n",
    "print(\"5. Deploy model as REST API\")\n",
    "print(\"6. Add model interpretability (Grad-CAM)\")\n",
    "print(\"7. Optimize for mobile deployment (TFLite)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training complete! ðŸŽ‰\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-and-use-header",
   "metadata": {},
   "source": [
    "## 16. How to Load and Use Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load saved model and make predictions\n",
    "\n",
    "# Load model\n",
    "loaded_model = load_model(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "\n",
    "print(\"\\nModel Metadata:\")\n",
    "print(f\"Model Type: {loaded_metadata['model_type']}\")\n",
    "print(f\"Classes: {loaded_metadata['class_labels']}\")\n",
    "print(f\"Best Accuracy: {loaded_metadata['performance']['best_val_accuracy']:.4f}\")\n",
    "\n",
    "# Make prediction with loaded model\n",
    "# result = predict_soil_type(\n",
    "#     'path/to/new/image.jpg',\n",
    "#     loaded_model,\n",
    "#     loaded_metadata['class_labels'],\n",
    "#     tuple(loaded_metadata['img_size'])\n",
    "# )\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
