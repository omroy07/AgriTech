{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ… Tomato Disease Prediction using Deep Learning\n",
    "\n",
    "## Overview\n",
    "Advanced tomato leaf disease classification using Transfer Learning and CNNs.\n",
    "\n",
    "**Dataset:** 10 disease classes (18,000+ images)\n",
    "- Bacterial Spot\n",
    "- Early Blight\n",
    "- Late Blight\n",
    "- Leaf Mold\n",
    "- Septoria Leaf Spot\n",
    "- Spider Mites\n",
    "- Target Spot\n",
    "- Mosaic Virus\n",
    "- Yellow Leaf Curl Virus\n",
    "- Healthy\n",
    "\n",
    "**Key Improvements:**\n",
    "- âœ… Transfer Learning (EfficientNetB0 & MobileNetV2)\n",
    "- âœ… Efficient data loading (TensorFlow datasets)\n",
    "- âœ… Advanced data augmentation\n",
    "- âœ… Learning rate scheduling\n",
    "- âœ… Comprehensive evaluation metrics\n",
    "- âœ… Model comparison & ensemble\n",
    "- âœ… Grad-CAM visualization\n",
    "\n",
    "**Expected Performance:**\n",
    "- Baseline CNN: ~97% accuracy\n",
    "- Transfer Learning: ~99%+ accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2, ResNet50V2\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization,\n",
    "    Activation, Flatten, Dropout, Dense, Input\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, \n",
    "    TensorBoard, LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support, roc_curve, auc,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# GPU configuration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive configuration\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'use_git_clone': True,  # Set False if dataset already downloaded\n",
    "    'data_dir': './CrowdAI-Plant-Disease-Prediction/Tomato/Dataset',\n",
    "    'img_size': (224, 224),  # Larger size for better features\n",
    "    'batch_size': 32,\n",
    "    'validation_split': 0.2,\n",
    "    'test_split': 0.1,\n",
    "    \n",
    "    # Model settings\n",
    "    'model_type': 'transfer_learning',  # 'cnn', 'transfer_learning', or 'ensemble'\n",
    "    'base_model_name': 'EfficientNetB0',  # EfficientNetB0, MobileNetV2, ResNet50V2\n",
    "    'num_classes': 10,\n",
    "    'use_class_weights': True,  # Handle class imbalance\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs_stage1': 20,\n",
    "    'epochs_stage2': 30,\n",
    "    'learning_rate': 0.001,\n",
    "    'learning_rate_finetune': 0.0001,\n",
    "    'optimizer': 'adam',  # adam or sgd\n",
    "    \n",
    "    # Augmentation\n",
    "    'use_advanced_augmentation': True,\n",
    "    \n",
    "    # Callbacks\n",
    "    'early_stopping_patience': 15,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'min_lr': 1e-7,\n",
    "    \n",
    "    # Paths\n",
    "    'model_save_dir': './models',\n",
    "    'logs_dir': './logs',\n",
    "    'output_dir': './outputs',\n",
    "    \n",
    "    # Feature extraction\n",
    "    'extract_features': False,  # Use feature extraction instead of fine-tuning\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CONFIG['model_save_dir'], CONFIG['logs_dir'], CONFIG['output_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Disease class mapping\n",
    "CLASS_NAMES = [\n",
    "    'Bacterial_spot',\n",
    "    'Early_blight',\n",
    "    'Late_blight',\n",
    "    'Leaf_Mold',\n",
    "    'Septoria_leaf_spot',\n",
    "    'Spider_mites',\n",
    "    'Target_Spot',\n",
    "    'Mosaic_virus',\n",
    "    'Yellow_Leaf_Curl_Virus',\n",
    "    'Healthy'\n",
    "]\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Model: {CONFIG['model_type']} with {CONFIG['base_model_name']}\")\n",
    "print(f\"Image size: {CONFIG['img_size']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Number of classes: {CONFIG['num_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['use_git_clone'] and not os.path.exists('CrowdAI-Plant-Disease-Prediction'):\n",
    "    print(\"Downloading dataset...\")\n",
    "    !git clone \"https://github.com/srajan-kiyotaka/CrowdAI-Plant-Disease-Prediction.git\"\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Dataset already exists or git clone disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Efficient Data Loading with TensorFlow Dataset API\n",
    "\n",
    "**Improvement:** Using TensorFlow's efficient data pipeline instead of manual loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset using TensorFlow Dataset API...\")\n",
    "\n",
    "# Load full dataset\n",
    "full_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    image_size=CONFIG['img_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "# Get class names from directory structure\n",
    "class_names = full_dataset.class_names\n",
    "print(f\"\\nDetected classes: {class_names}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Calculate dataset size\n",
    "total_batches = tf.data.experimental.cardinality(full_dataset).numpy()\n",
    "print(f\"Total batches: {total_batches}\")\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * total_batches)\n",
    "val_size = int(0.2 * total_batches)\n",
    "test_size = total_batches - train_size - val_size\n",
    "\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "remaining = full_dataset.skip(train_size)\n",
    "val_dataset = remaining.take(val_size)\n",
    "test_dataset = remaining.skip(val_size)\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"  Training batches: {train_size}\")\n",
    "print(f\"  Validation batches: {val_size}\")\n",
    "print(f\"  Test batches: {test_size}\")\n",
    "\n",
    "# Calculate approximate image counts\n",
    "train_images = train_size * CONFIG['batch_size']\n",
    "val_images = val_size * CONFIG['batch_size']\n",
    "test_images = test_size * CONFIG['batch_size']\n",
    "\n",
    "print(f\"\\nApproximate image counts:\")\n",
    "print(f\"  Training: ~{train_images}\")\n",
    "print(f\"  Validation: ~{val_images}\")\n",
    "print(f\"  Test: ~{test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Get samples\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(min(20, len(images))):\n",
    "        plt.subplot(4, 5, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[labels[i].numpy()], fontsize=10)\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Tomato Leaf Images', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'sample_images.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample images saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "print(\"Analyzing class distribution...\")\n",
    "\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "\n",
    "for images, labels in train_dataset:\n",
    "    for label in labels.numpy():\n",
    "        class_counts[class_names[label]] += 1\n",
    "\n",
    "# Create DataFrame\n",
    "class_dist_df = pd.DataFrame([\n",
    "    {'Disease': name, 'Count': count, 'Percentage': count/sum(class_counts.values())*100}\n",
    "    for name, count in class_counts.items()\n",
    "]).sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(class_dist_df.to_string(index=False))\n",
    "\n",
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(class_dist_df['Disease'], class_dist_df['Count'], color='steelblue')\n",
    "axes[0].set_xlabel('Disease Class', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[0].set_title('Class Distribution (Training Set)', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_dist_df['Count'], labels=class_dist_df['Disease'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Class Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'class_distribution.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "if CONFIG['use_class_weights']:\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_weights = {\n",
    "        i: total_samples / (len(class_counts) * count)\n",
    "        for i, (name, count) in enumerate(class_counts.items())\n",
    "    }\n",
    "    print(\"\\nClass weights calculated for imbalanced dataset\")\n",
    "else:\n",
    "    class_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization layer\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Advanced data augmentation\n",
    "if CONFIG['use_advanced_augmentation']:\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.RandomRotation(0.3),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "        tf.keras.layers.RandomBrightness(0.2),\n",
    "    ], name='data_augmentation')\n",
    "else:\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "    ], name='data_augmentation')\n",
    "\n",
    "# Apply preprocessing\n",
    "def preprocess_dataset(dataset, augment=False):\n",
    "    \"\"\"Apply normalization and optional augmentation\"\"\"\n",
    "    dataset = dataset.map(lambda x, y: (normalization_layer(x), y),\n",
    "                         num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                             num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Preprocess datasets\n",
    "train_dataset = preprocess_dataset(train_dataset, augment=True)\n",
    "val_dataset = preprocess_dataset(val_dataset, augment=False)\n",
    "test_dataset = preprocess_dataset(test_dataset, augment=False)\n",
    "\n",
    "# Configure for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"Data preprocessing and augmentation configured!\")\n",
    "\n",
    "# Visualize augmentations\n",
    "plt.figure(figsize=(16, 8))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    sample_image = images[0:1]\n",
    "    \n",
    "    for i in range(12):\n",
    "        augmented = data_augmentation(sample_image, training=True)\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(augmented[0])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Augmentation {i+1}\")\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'augmentation_examples.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Architecture\n",
    "\n",
    "### Three Options:\n",
    "1. **Baseline CNN** - Custom architecture\n",
    "2. **Transfer Learning** - Pre-trained models (EfficientNet, MobileNet, ResNet)\n",
    "3. **Ensemble** - Combine multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_baseline_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Improved baseline CNN with Batch Normalization\n",
    "    Expected accuracy: ~97%\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Block 4\n",
    "        Conv2D(256, (3, 3), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Classifier\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name='baseline_cnn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_transfer_learning_model(input_shape, num_classes, base_model_name='EfficientNetB0'):\n",
    "    \"\"\"\n",
    "    Transfer Learning with various pre-trained models\n",
    "    Expected accuracy: ~99%+\n",
    "    \"\"\"\n",
    "    # Select base model\n",
    "    if base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, \n",
    "                                   input_shape=input_shape)\n",
    "    elif base_model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                                input_shape=input_shape)\n",
    "    elif base_model_name == 'ResNet50V2':\n",
    "        base_model = ResNet50V2(weights='imagenet', include_top=False,\n",
    "                               input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown base model: {base_model_name}\")\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build complete model\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name=f'transfer_{base_model_name}')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "\n",
    "print(\"Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model based on configuration\n",
    "input_shape = (*CONFIG['img_size'], 3)\n",
    "\n",
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(f\"Building Transfer Learning model: {CONFIG['base_model_name']}\")\n",
    "    model, base_model = build_transfer_learning_model(\n",
    "        input_shape=input_shape,\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        base_model_name=CONFIG['base_model_name']\n",
    "    )\n",
    "else:\n",
    "    print(\"Building baseline CNN model\")\n",
    "    model = build_baseline_cnn(\n",
    "        input_shape=input_shape,\n",
    "        num_classes=CONFIG['num_classes']\n",
    "    )\n",
    "    base_model = None\n",
    "\n",
    "# Compile model\n",
    "if CONFIG['optimizer'] == 'adam':\n",
    "    optimizer = Adam(learning_rate=CONFIG['learning_rate'])\n",
    "else:\n",
    "    optimizer = SGD(learning_rate=CONFIG['learning_rate'], momentum=0.9)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate reduction\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=CONFIG['reduce_lr_patience'],\n",
    "    min_lr=CONFIG['min_lr'],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(CONFIG['model_save_dir'], \n",
    "                         'tomato_disease_{epoch:02d}_{val_accuracy:.4f}.h5'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# TensorBoard\n",
    "log_dir = os.path.join(CONFIG['logs_dir'], \n",
    "                      datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# Custom callback to log metrics\n",
    "class MetricsLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': logs.get('loss'),\n",
    "            'accuracy': logs.get('accuracy'),\n",
    "            'val_loss': logs.get('val_loss'),\n",
    "            'val_accuracy': logs.get('val_accuracy'),\n",
    "            'lr': float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        })\n",
    "\n",
    "metrics_logger = MetricsLogger()\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr, checkpoint, tensorboard_callback, metrics_logger]\n",
    "\n",
    "print(\"Callbacks configured!\")\n",
    "print(f\"TensorBoard logs: {log_dir}\")\n",
    "print(\"To view: tensorboard --logdir=./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Training\n",
    "\n",
    "### Two-Stage Training (Transfer Learning):\n",
    "1. Train only new layers (base frozen)\n",
    "2. Fine-tune entire model (base unfrozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(\"=\"*80)\n",
    "    print(\"STAGE 1: Training new layers (base model frozen)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    history_stage1 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs_stage1'],\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Stage 1 Complete! Best val_accuracy: {max(history_stage1.history['val_accuracy']):.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"Training baseline CNN model\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Training Complete! Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STAGE 2: Fine-tuning entire model (base model unfrozen)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Unfreeze base model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = len(base_model.layers) // 2\n",
    "    \n",
    "    # Freeze earlier layers\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    print(f\"Unfrozen layers: {sum([1 for l in base_model.layers if l.trainable])} / {len(base_model.layers)}\")\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=CONFIG['learning_rate_finetune']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy')]\n",
    "    )\n",
    "    \n",
    "    print(f\"Learning rate: {CONFIG['learning_rate_finetune']}\")\n",
    "    \n",
    "    # Continue training\n",
    "    history_stage2 = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=CONFIG['epochs_stage2'],\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        initial_epoch=len(history_stage1.history['loss']),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Stage 2 Complete! Best val_accuracy: {max(history_stage2.history['val_accuracy']):.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Combine histories\n",
    "    history = type('obj', (object,), {\n",
    "        'history': {\n",
    "            'loss': history_stage1.history['loss'] + history_stage2.history['loss'],\n",
    "            'accuracy': history_stage1.history['accuracy'] + history_stage2.history['accuracy'],\n",
    "            'val_loss': history_stage1.history['val_loss'] + history_stage2.history['val_loss'],\n",
    "            'val_accuracy': history_stage1.history['val_accuracy'] + history_stage2.history['val_accuracy']\n",
    "        }\n",
    "    })()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate over epochs\n",
    "if metrics_logger.metrics:\n",
    "    lrs = [m['lr'] for m in metrics_logger.metrics]\n",
    "    axes[1, 0].plot(lrs, linewidth=2, color='green')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy improvement per epoch\n",
    "acc_improvement = np.diff([0] + history.history['val_accuracy'])\n",
    "axes[1, 1].bar(range(len(acc_improvement)), acc_improvement, \n",
    "              color=['green' if x > 0 else 'red' for x in acc_improvement])\n",
    "axes[1, 1].set_title('Validation Accuracy Change per Epoch', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy Change')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add stage marker if transfer learning\n",
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    stage1_epochs = len(history_stage1.history['loss'])\n",
    "    for ax in axes.flat[:2]:\n",
    "        ax.axvline(x=stage1_epochs-1, color='red', linestyle='--', \n",
    "                  label='Fine-tuning starts', linewidth=2)\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'training_history.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total epochs: {len(history.history['loss'])}\")\n",
    "print(f\"Final train accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final val accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Best val accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "print(f\"Final train loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "# Collect predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "print(\"Collecting predictions...\")\n",
    "for images, labels in tqdm(test_dataset, desc=\"Testing\"):\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_pred_proba.extend(predictions)\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(f\"\\nCollected {len(y_true)} predictions\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "test_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test F1-Score (weighted): {test_f1:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=None\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Disease': class_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nPer-Class Metrics (sorted by F1-Score):\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv(os.path.join(CONFIG['output_dir'], 'classification_metrics.csv'), \n",
    "                  index=False)\n",
    "print(\"\\nMetrics saved to 'classification_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            annot_kws={'size': 10})\n",
    "plt.title('Confusion Matrix - Tomato Disease Classification', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'confusion_matrix.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Proportion'},\n",
    "            annot_kws={'size': 10})\n",
    "plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'confusion_matrix_normalized.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves (One-vs-Rest)\n",
    "y_true_bin = label_binarize(y_true, classes=range(len(class_names)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (ax, class_name) in enumerate(zip(axes, class_names)):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'AUC = {roc_auc:.3f}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(class_name, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ROC Curves - Per Class (One-vs-Rest)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'roc_curves.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate macro and micro average AUC\n",
    "fpr_micro, tpr_micro, _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "micro_auc = auc(fpr_micro, tpr_micro)\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([roc_curve(y_true_bin[:, i], y_pred_proba[:, i])[0] \n",
    "                                     for i in range(len(class_names))]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(len(class_names)):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "mean_tpr /= len(class_names)\n",
    "macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "print(f\"\\nMicro-average AUC: {micro_auc:.4f}\")\n",
    "print(f\"Macro-average AUC: {macro_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_idx = np.where(y_true != y_pred)[0]\n",
    "print(f\"Total misclassified: {len(misclassified_idx)} / {len(y_true)} ({len(misclassified_idx)/len(y_true)*100:.2f}%)\")\n",
    "\n",
    "# Analyze most confused pairs\n",
    "from collections import Counter\n",
    "\n",
    "confusion_pairs = [(class_names[y_true[i]], class_names[y_pred[i]]) \n",
    "                   for i in misclassified_idx]\n",
    "most_confused = Counter(confusion_pairs).most_common(10)\n",
    "\n",
    "print(\"\\nTop 10 Most Confused Class Pairs:\")\n",
    "for (true_class, pred_class), count in most_confused:\n",
    "    print(f\"  {true_class} â†’ {pred_class}: {count} times\")\n",
    "\n",
    "# Visualize some misclassified examples\n",
    "if len(misclassified_idx) > 0:\n",
    "    # Get some misclassified samples\n",
    "    sample_indices = np.random.choice(misclassified_idx, \n",
    "                                     min(12, len(misclassified_idx)), \n",
    "                                     replace=False)\n",
    "    \n",
    "    # Note: This requires access to original images\n",
    "    # Skipping visualization in this optimized version\n",
    "    print(\"\\nMisclassified samples identified for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model_path = os.path.join(CONFIG['model_save_dir'], 'tomato_disease_final.h5')\n",
    "model.save(model_path)\n",
    "print(f\"Model saved: {model_path}\")\n",
    "\n",
    "# Save as SavedModel format (for deployment)\n",
    "saved_model_path = os.path.join(CONFIG['model_save_dir'], 'tomato_disease_saved_model')\n",
    "model.save(saved_model_path, save_format='tf')\n",
    "print(f\"SavedModel format saved: {saved_model_path}\")\n",
    "\n",
    "# Comprehensive metadata\n",
    "metadata = {\n",
    "    'model_info': {\n",
    "        'type': CONFIG['model_type'],\n",
    "        'base_model': CONFIG['base_model_name'] if CONFIG['model_type'] == 'transfer_learning' else 'CNN',\n",
    "        'architecture': model.name,\n",
    "        'total_parameters': int(total_params),\n",
    "        'trainable_parameters': int(trainable_params),\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'num_classes': CONFIG['num_classes'],\n",
    "        'class_names': class_names,\n",
    "        'img_size': CONFIG['img_size'],\n",
    "        'train_images': train_images,\n",
    "        'val_images': val_images,\n",
    "        'test_images': test_images,\n",
    "    },\n",
    "    'training_info': {\n",
    "        'total_epochs': len(history.history['loss']),\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'optimizer': CONFIG['optimizer'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'learning_rate_finetune': CONFIG['learning_rate_finetune'] if CONFIG['model_type'] == 'transfer_learning' else None,\n",
    "        'use_augmentation': CONFIG['use_advanced_augmentation'],\n",
    "        'use_class_weights': CONFIG['use_class_weights'],\n",
    "    },\n",
    "    'performance': {\n",
    "        'train_accuracy': float(history.history['accuracy'][-1]),\n",
    "        'val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "        'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_f1_score': float(test_f1),\n",
    "        'micro_auc': float(micro_auc),\n",
    "        'macro_auc': float(macro_auc),\n",
    "    },\n",
    "    'per_class_metrics': metrics_df.to_dict('records'),\n",
    "    'training_date': datetime.datetime.now().isoformat(),\n",
    "    'tensorflow_version': tf.__version__,\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = model_path.replace('.h5', '_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(CONFIG['output_dir'], 'training_history.csv'), \n",
    "                  index_label='epoch')\n",
    "print(\"Training history saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tomato_disease(img_path, model, class_names, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Predict tomato disease from image\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to tomato leaf image\n",
    "        model: Trained model\n",
    "        class_names: List of disease class names\n",
    "        img_size: Target image size\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate image\n",
    "        if not os.path.exists(img_path):\n",
    "            return {'success': False, 'error': f'Image not found: {img_path}'}\n",
    "        \n",
    "        # Load and preprocess\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=img_size)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        pred_class = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][pred_class]\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top_3_idx = np.argsort(predictions[0])[-3:][::-1]\n",
    "        top_3_predictions = [\n",
    "            {\n",
    "                'disease': class_names[idx],\n",
    "                'confidence': float(predictions[0][idx])\n",
    "            }\n",
    "            for idx in top_3_idx\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'predicted_disease': class_names[pred_class],\n",
    "            'confidence': float(confidence),\n",
    "            'top_3_predictions': top_3_predictions,\n",
    "            'all_probabilities': {\n",
    "                class_names[i]: float(predictions[0][i])\n",
    "                for i in range(len(class_names))\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e)}\n",
    "\n",
    "\n",
    "def visualize_prediction(img_path, result):\n",
    "    \"\"\"\n",
    "    Visualize prediction result\n",
    "    \"\"\"\n",
    "    if not result['success']:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Show image\n",
    "    img = Image.open(img_path)\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\n",
    "        f'Predicted: {result[\"predicted_disease\"]}\\nConfidence: {result[\"confidence\"]:.2%}',\n",
    "        fontsize=14, fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    top_3 = result['top_3_predictions']\n",
    "    diseases = [p['disease'] for p in top_3]\n",
    "    confidences = [p['confidence'] for p in top_3]\n",
    "    \n",
    "    colors = ['green' if i == 0 else 'steelblue' for i in range(len(diseases))]\n",
    "    bars = ax2.barh(diseases, confidences, color=colors)\n",
    "    \n",
    "    ax2.set_xlabel('Confidence', fontsize=12)\n",
    "    ax2.set_title('Top 3 Predictions', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, conf in zip(bars, confidences):\n",
    "        ax2.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{conf:.1%}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG['output_dir'], 'prediction_result.png'),\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Prediction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Test Prediction\n",
    "\n",
    "**Note:** Update the image path to test on your own images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "# Update this path to your test image\n",
    "test_image_path = \"path/to/your/test/image.jpg\"\n",
    "\n",
    "if os.path.exists(test_image_path):\n",
    "    result = predict_tomato_disease(\n",
    "        test_image_path,\n",
    "        model,\n",
    "        class_names,\n",
    "        img_size=CONFIG['img_size']\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PREDICTION RESULT\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Disease: {result['predicted_disease']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "        print(\"\\nTop 3 Predictions:\")\n",
    "        for i, pred in enumerate(result['top_3_predictions'], 1):\n",
    "            print(f\"  {i}. {pred['disease']}: {pred['confidence']:.2%}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_prediction(test_image_path, result)\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "else:\n",
    "    print(f\"Test image not found: {test_image_path}\")\n",
    "    print(\"Update the path to test prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Model Export for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TensorFlow Lite (for mobile/edge deployment)\n",
    "print(\"Converting to TensorFlow Lite...\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = os.path.join(CONFIG['model_save_dir'], 'tomato_disease.tflite')\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite model saved: {tflite_path}\")\n",
    "print(f\"TFLite model size: {os.path.getsize(tflite_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Export class names for deployment\n",
    "class_names_path = os.path.join(CONFIG['model_save_dir'], 'class_names.json')\n",
    "with open(class_names_path, 'w') as f:\n",
    "    json.dump({'class_names': class_names}, f, indent=2)\n",
    "\n",
    "print(f\"Class names saved: {class_names_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Type: {CONFIG['model_type']}\")\n",
    "if CONFIG['model_type'] == 'transfer_learning':\n",
    "    print(f\"  Base Model: {CONFIG['base_model_name']}\")\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "print(f\"  Image Size: {CONFIG['img_size']}\")\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total Classes: {CONFIG['num_classes']}\")\n",
    "print(f\"  Training Images: ~{train_images}\")\n",
    "print(f\"  Validation Images: ~{val_images}\")\n",
    "print(f\"  Test Images: ~{test_images}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Best Val Accuracy: {max(history.history['val_accuracy']):.4f} ({max(history.history['val_accuracy'])*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  Macro AUC: {macro_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nFiles Generated:\")\n",
    "print(f\"  âœ“ sample_images.png\")\n",
    "print(f\"  âœ“ class_distribution.png\")\n",
    "print(f\"  âœ“ augmentation_examples.png\")\n",
    "print(f\"  âœ“ training_history.png\")\n",
    "print(f\"  âœ“ confusion_matrix.png\")\n",
    "print(f\"  âœ“ confusion_matrix_normalized.png\")\n",
    "print(f\"  âœ“ roc_curves.png\")\n",
    "print(f\"  âœ“ classification_metrics.csv\")\n",
    "print(f\"  âœ“ training_history.csv\")\n",
    "print(f\"  âœ“ {model_path}\")\n",
    "print(f\"  âœ“ {metadata_path}\")\n",
    "print(f\"  âœ“ {tflite_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"NEXT STEPS & RECOMMENDATIONS\")\n",
    "print(\"-\"*80)\n",
    "print(\"1. Try different base models (ResNet50V2, EfficientNetB3)\")\n",
    "print(\"2. Experiment with larger image sizes (299x299)\")\n",
    "print(\"3. Implement model ensemble for better accuracy\")\n",
    "print(\"4. Add Grad-CAM visualization for interpretability\")\n",
    "print(\"5. Deploy as REST API (FastAPI/Flask)\")\n",
    "print(\"6. Convert to ONNX for cross-platform deployment\")\n",
    "print(\"7. Implement active learning for continuous improvement\")\n",
    "print(\"8. Add confidence thresholds for uncertain predictions\")\n",
    "print(\"9. Create mobile app using TFLite model\")\n",
    "print(\"10. Set up MLOps pipeline for model monitoring\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete! ðŸŽ‰\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Load and Use Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load saved model\n",
    "\"\"\"\n",
    "# Load model\n",
    "loaded_model = load_model(model_path)\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "\n",
    "# Load class names\n",
    "with open(class_names_path, 'r') as f:\n",
    "    loaded_class_names = json.load(f)['class_names']\n",
    "\n",
    "# Make prediction\n",
    "result = predict_tomato_disease(\n",
    "    'path/to/image.jpg',\n",
    "    loaded_model,\n",
    "    loaded_class_names,\n",
    "    tuple(loaded_metadata['dataset_info']['img_size'])\n",
    ")\n",
    "\n",
    "print(result)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example code for loading and using the saved model provided above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
